{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 tries left\n",
    "2 karun\n",
    "4 haoyu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import multiprocessing \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ = \"r\"\n",
    "# stores the mapping from a person to the people they follow\n",
    "follows_original = {}\n",
    "follows = {}\n",
    "unique_ids = set({})\n",
    "follow_counts = {}\n",
    "# stores deleted edges from the training data\n",
    "dev_list = []\n",
    "positive_edges_list = []\n",
    "negative_edges_list = []\n",
    "\n",
    "# --------- Hyper-parameters ----------- #\n",
    "RANDOM_WALK_LENGTH = 10\n",
    "WALKS_PER_NODE = 10000\n",
    "# -------------------------------------- #\n",
    "\n",
    "# TODO change the path\n",
    "TRAINING_FILE_PATH = '../../Data/train.txt'\n",
    "#MODEL_PATH = '../data/rw.model'\n",
    "#WORD_VECTORS_PATH = '../data/word_vectors'\n",
    "#WORD_VECTOR_MATRIX = \"../data/rw3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the follows data-structure\n",
    "def load_training_data():\n",
    "    train_data = open(TRAINING_FILE_PATH, READ)\n",
    "    for entry in train_data:\n",
    "        ids = entry.strip().split('\\t')\n",
    "        # add all the entries to the SET to find the unique ids in the training data\n",
    "        unique_ids.update(ids)\n",
    "\n",
    "        # extract the source\n",
    "        source = ids[0]\n",
    "        # extract the people who the user follows, by removing the user-id from the ids\n",
    "        sinks = set(ids[1:])\n",
    "        print\n",
    "        \n",
    "        # follows original \n",
    "        follows_original[source] = sinks\n",
    "        \n",
    "        # creating deleted edges from training data\n",
    "        if len(sinks) >= 2:\n",
    "            del_sink = random.choice(tuple(sinks))\n",
    "            positive_edges_list.append((source,del_sink,1))\n",
    "            negative_edges_list.append((source, random.choice(range(500)),0))\n",
    "            # removing the sink nodes from training data\n",
    "            sinks = sinks - set(del_sink)\n",
    "            \n",
    "        \n",
    "        # maps user to the people they follow\n",
    "        follows[source] = sinks\n",
    "\n",
    "        # keep a map of no of people they follow\n",
    "        follow_counts[source] = len(sinks)\n",
    "        \n",
    "    #df = pd.DataFrame({})\n",
    "    dev_list.extend(positive_edges_list[:5000])\n",
    "    dev_list.extend(negative_edges_list[:5000])\n",
    "    \n",
    "    with open('pickle_files/follows.pickle', 'wb') as handle:\n",
    "        pickle.dump(follows, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #dataframe dev data\n",
    "    df_gt = pd.DataFrame(dev_list,columns=[\"Source\",\"Sink\",\"State\"])\n",
    "    df = df_gt[df_gt.columns.difference([\"State\"])]\n",
    "    \n",
    "    with open('pickle_files/df.pickle', 'wb') as handle:\n",
    "        pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('pickle_files/df_gt.pickle', 'wb') as handle:\n",
    "        pickle.dump(df_gt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating negative samples for dev set\n",
    "\n",
    "#negative_edges_list =[]\n",
    "#for i in range(5000):\n",
    "#item = random.sample(follows_original.keys(),5000)\n",
    "#print(len(item))\n",
    "#print(item)\n",
    "#for i in item:\n",
    "    #n_item = tuple(unique_ids - follows_original[i])\n",
    "    #n_node = random.choice(n_item)\n",
    "    #negative_edges_list.append((i,n_node,\"False\"))\n",
    "#print(negative_edges_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({})\n",
    "#dev_list.extend(positive_edges_list[:5000])\n",
    "#dev_list.extend(negative_edges_list)\n",
    "#dataframe dev data\n",
    "#df_gt = pd.DataFrame(dev_list,columns=[\"Source\",\"Sink\",\"State\"])\n",
    "#df = df_gt[df_gt.columns.difference([\"State\"])]\n",
    "#print(len(df))\n",
    "#print(len(df_gt))\n",
    "#with open('pickle_files/df.pickle', 'wb') as handle:\n",
    "    #pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('pickle_files/df_gt.pickle', 'wb') as handle:\n",
    "    #pickle.dump(df_gt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs a single random walk on the passed node for the given number of hops\n",
    "# please note that this walk is ALWAYS from a source to a sink\n",
    "def perform_random_walk(node):\n",
    "    \n",
    "    # stores the path of a single random walk\n",
    "    random_walk = [node]\n",
    "    hops = RANDOM_WALK_LENGTH\n",
    "    while hops >= 0:\n",
    "        hops -= 1\n",
    "        if node in follows:\n",
    "            neighbours = list(follows[node])\n",
    "            # remove the neighbours already visited\n",
    "            #neighbours = list(set(neighbours) - set(random_walk))\n",
    "            if len(neighbours) == 0:\n",
    "                break\n",
    "        else:\n",
    "            # cannot go any further\n",
    "            break\n",
    "\n",
    "        # choose one of the neighbouring nodes\n",
    "        random_node = random.choice(neighbours)\n",
    "        random_walk.append(random_node)\n",
    "        # jump to the new node\n",
    "        node = random_node\n",
    "\n",
    "    return random_walk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an array of random walks\n",
    "# they all are NOT of the same length\n",
    "def get_random_walks():\n",
    "    random_walks = []\n",
    "    # perform random walks for all nodes who follow somebody\n",
    "    for unique_node in tqdm(unique_nodes):\n",
    "        if unique_node in follows:\n",
    "            for index in range(min(WALKS_PER_NODE, follow_counts[unique_node])*4):\n",
    "                random_walks.append(perform_random_walk(unique_node))\n",
    "    #with open('../data/random_walks_4x.pickle', 'wb') as handle:\n",
    "    #    pickle.dump(random_walks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return random_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_model(random_walks):\n",
    "    '''\n",
    "    model = Word2Vec(size=300, workers = 5, alpha=0.03, window=4, sg=1, hs=0,\n",
    "                     negative=10, ns_exponent = 0.75, min_alpha=0.0005,\n",
    "                     seed=99)\n",
    "    '''\n",
    "    model = Word2Vec(size=400, workers = 5, alpha=0.025, window=3, sg=1, hs=1,\n",
    "                     negative=10, min_alpha=0.0005,\n",
    "                     seed=99)\n",
    "    \n",
    "    model.build_vocab(random_walks)\n",
    "    model.train(random_walks, total_examples=model.corpus_count, epochs=10)\n",
    "    model.save(MODEL_PATH)\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    model.wv.save_word2vec_format(WORD_VECTOR_MATRIX)\n",
    "    model.wv.save(WORD_VECTORS_PATH)\n",
    "\n",
    "    # perform some sanity checks\n",
    "    print(model.similar_by_word('540762'))\n",
    "    print(model.wv.most_similar(positive=[\"540762\"]))\n",
    "    print(model.wv.similarity(\"540762\", '2345899'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = list(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walks = get_random_walks()\n",
    "print(len(random_walks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follows = {}\n",
    "unique_ids = set({})\n",
    "follow_counts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/random_walks_4x.pickle', 'rb') as handle:\n",
    "    random_walks = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(random_walks)\n",
    "for i in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_model(random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "\n",
    "TRAINING_FILE_PATH = '../data/train.txt'\n",
    "TESTING_FILE_PATH = '../data/test-public.txt'\n",
    "READ = \"r\"\n",
    "WRITE = \"w\"\n",
    "OUT_FILE_PATH = 'current_best.csv'\n",
    "\n",
    "outfile = open(OUT_FILE_PATH, WRITE)\n",
    "\n",
    "# Load the word2vec model\n",
    "model = Word2Vec.load(\"../data/rw.model\")\n",
    "train_data = open(TRAINING_FILE_PATH, READ)\n",
    "\n",
    "# None marker, indicates that processing did not happen\n",
    "NULL = -5\n",
    "\n",
    "outfile.write('Id,Predicted' + '\\n')\n",
    "\n",
    "sink2sources = {}\n",
    "source2sinks = {}\n",
    "source2sink_counts = {}\n",
    "\n",
    "\n",
    "# compares the similarity of source,\n",
    "def get_source_as_sink_similarity(source, sink):\n",
    "\n",
    "    connected_sinks = source2sinks[source]\n",
    "\n",
    "    c_sinks = 0\n",
    "    total_sim = 0\n",
    "\n",
    "    for connected_sink in connected_sinks:\n",
    "        if connected_sink in model.wv.vocab:\n",
    "            c_sinks += 1\n",
    "            # print('-----', model.wv.similarity(connected_sink, sink))\n",
    "            total_sim = total_sim + model.wv.similarity(connected_sink, sink)\n",
    "\n",
    "    # print(c_sinks)\n",
    "\n",
    "    if c_sinks > 0:\n",
    "        return total_sim/c_sinks\n",
    "    else:\n",
    "        return -5\n",
    "\n",
    "\n",
    "def get_sink_as_source_similarity(source, sink):\n",
    "\n",
    "    connected_sources = sink2sources[sink]\n",
    "\n",
    "    c_sources = 0\n",
    "    total_sim = 0\n",
    "\n",
    "    for connected_source in connected_sources:\n",
    "        if connected_source in model.wv.vocab:\n",
    "            c_sources += 1\n",
    "            total_sim = total_sim + model.wv.similarity(connected_source, source)\n",
    "\n",
    "    # print(c_sinks)\n",
    "\n",
    "    if c_sources > 0:\n",
    "        return total_sim/c_sources\n",
    "    else:\n",
    "        return NULL\n",
    "\n",
    "\n",
    "# returns a representation for source\n",
    "def get_representation_for_source(src):\n",
    "\n",
    "    if src in model.wv.vocab:\n",
    "        word_vector_rep = model.wv[src]\n",
    "        # print(word_vector_rep)\n",
    "    elif src in source2sinks and len(source2sinks[src]) > 0:\n",
    "        # if the source is missing, but the source has some sinks in the train set\n",
    "        # we then say that a source is defined by the avg of the sources which follows its sinks\n",
    "        # need to weigh this in future !\n",
    "        print('No Match', src)\n",
    "        print(len(source2sinks[src]))\n",
    "\n",
    "        connected_sinks = source2sinks[src]\n",
    "        c_sinks = 0\n",
    "        for sink in connected_sinks:\n",
    "            if sink in model.wv.vocab:\n",
    "                c_sinks += 1\n",
    "\n",
    "        print(c_sinks)\n",
    "    else:\n",
    "        print('NOTHING', src)\n",
    "\n",
    "\n",
    "# returns a rep for sink\n",
    "def get_representation_for_sink(snk):\n",
    "    if snk in model.wv.vocab:\n",
    "        word_vector_rep = model.wv[snk]\n",
    "        # print(word_vector_rep)\n",
    "    else:\n",
    "        print('No word vector', snk)\n",
    "\n",
    "\n",
    "# loads the source to sinks and sink to sources map\n",
    "def load_mappings():\n",
    "    for entry in train_data:\n",
    "        ids = entry.strip().split('\\t')\n",
    "        source = ids[0].strip()\n",
    "        sinks = set(ids[1:])\n",
    "        source2sinks[source] = sinks\n",
    "        source2sink_counts[source] = len(sinks)\n",
    "        for sink in sinks:\n",
    "            if sink in sink2sources:\n",
    "                sink2sources[sink.strip()].append(source)\n",
    "            else:\n",
    "                sink2sources[sink.strip()] = [source]\n",
    "\n",
    "\n",
    "def predict_with_test_data():\n",
    "\n",
    "    test_data = open(TESTING_FILE_PATH, READ)\n",
    "    test_data.readline()\n",
    "    no_match = 0\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for line in tqdm(test_data):\n",
    "        index += 1\n",
    "        data = line.split('\\t')\n",
    "        source, sink = data[1].strip(), data[2].strip()\n",
    "        direct_sim = NULL\n",
    "        source_as_sink_sim = NULL\n",
    "        sink_as_source_sim = NULL\n",
    "        total = 1000002\n",
    "\n",
    "        # if both source and sink have word vector representations\n",
    "        if source in model.wv.vocab and sink in model.wv.vocab:\n",
    "            direct_sim = model.wv.similarity(source, sink)\n",
    "\n",
    "        # transform source as a fn of the sinks it follows and then calculate the similarity between the two\n",
    "        if sink in model.wv.vocab:\n",
    "            source_as_sink_sim = get_source_as_sink_similarity(source, sink)\n",
    "\n",
    "        if source in model.wv.vocab:\n",
    "            sink_as_source_sim = get_sink_as_source_similarity(source, sink)\n",
    "\n",
    "        if direct_sim == NULL and source_as_sink_sim == NULL and sink_as_source_sim == NULL:\n",
    "            no_match += 1\n",
    "\n",
    "        if direct_sim == NULL:\n",
    "            total = total - 1\n",
    "        if source_as_sink_sim == NULL:\n",
    "            total = total - 1\n",
    "        if sink_as_source_sim == NULL:\n",
    "            print('error')\n",
    "            #sys.exit(0)\n",
    "            total = total - 1000000\n",
    "\n",
    "        total_probability = max(0, direct_sim) + max(0, source_as_sink_sim) + max(0, sink_as_source_sim) * 1000000\n",
    "        # total_probability = max(0, sink_as_source_sim) * 1000\n",
    "        \n",
    "        if total == 0:\n",
    "            total_probability = 0.1\n",
    "        else:\n",
    "            total_probability = total_probability / total\n",
    "\n",
    "        print(direct_sim, ' : ', source_as_sink_sim, ' : ', sink_as_source_sim, ' : ', total_probability)\n",
    "        outfile.write(str(index) + ',' + str(total_probability) + '\\n')\n",
    "\n",
    "    print(no_match)\n",
    "\n",
    "\n",
    "load_mappings()\n",
    "predict_with_test_data()\n",
    "outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
